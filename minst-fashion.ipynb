{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "def prep_data(raw):\n",
    "    y = raw[:, 0]\n",
    "    out_y = keras.utils.to_categorical(y, num_classes)\n",
    "    \n",
    "    x = raw[:,1:]\n",
    "    num_images = raw.shape[0]\n",
    "    out_x = x.reshape(num_images, img_rows, img_cols, 1)\n",
    "    out_x = out_x / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "fashion_file = \"../input/fashionmnist/fashion-mnist_train.csv\"\n",
    "fashion_data = np.loadtxt(fashion_file, skiprows=1, delimiter=',')\n",
    "x, y = prep_data(fashion_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1) Start the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "fashion_model= Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2) Add the first layer\n",
    "\n",
    "Add the first `Conv2D` layer to `fashion_model`. It  have 12 filters, a kernel_size of 3 and the `relu` activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "fashion_model.add(Conv2D(12,\n",
    "                         activation='relu',\n",
    "                         kernel_size=3, \n",
    "                         input_shape = (img_rows, img_cols, 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3) Add the remaining layers\n",
    "\n",
    "1. 2 more convolutional (`Conv2D layers`) with 20 filters each, 'relu' activation, and a kernel size of 3. a `Flatten` layer, and then a `Dense` layer with 100 neurons. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "fashion_model.add(Conv2D(20, kernel_size =3,activation='relu') \n",
    ")\n",
    "fashion_model.add(Conv2D(20, kernel_size =3,activation='relu') \n",
    ")\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(100, activation = 'relu'))\n",
    "fashion_model.add(Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_3.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 4) Compile  Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to compile the model in this cell\n",
    "fashion_model.compile(loss= 'categorical_crossentropy',optimizer ='adam', metrics= 'accuracy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 5) Fit The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "480/480 [==============================] - 3s 5ms/step - loss: 0.4770 - accuracy: 0.8282 - val_loss: 0.3372 - val_accuracy: 0.8792\n",
      "Epoch 2/4\n",
      "480/480 [==============================] - 2s 5ms/step - loss: 0.3049 - accuracy: 0.8912 - val_loss: 0.2937 - val_accuracy: 0.8954\n",
      "Epoch 3/4\n",
      "480/480 [==============================] - 2s 5ms/step - loss: 0.2528 - accuracy: 0.9089 - val_loss: 0.2772 - val_accuracy: 0.9033\n",
      "Epoch 4/4\n",
      "480/480 [==============================] - 2s 5ms/step - loss: 0.2137 - accuracy: 0.9215 - val_loss: 0.2685 - val_accuracy: 0.9075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fec730f80d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code to fit the model here\n",
    "fashion_model.fit(x, y, batch_size=100, epochs=4, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 6) Different approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "480/480 [==============================] - 3s 6ms/step - loss: 0.4901 - accuracy: 0.8243 - val_loss: 0.3484 - val_accuracy: 0.8773\n",
      "Epoch 2/6\n",
      "480/480 [==============================] - 2s 5ms/step - loss: 0.3098 - accuracy: 0.8879 - val_loss: 0.3290 - val_accuracy: 0.8795\n",
      "Epoch 3/6\n",
      "480/480 [==============================] - 2s 5ms/step - loss: 0.2513 - accuracy: 0.9089 - val_loss: 0.2807 - val_accuracy: 0.9014\n",
      "Epoch 4/6\n",
      "480/480 [==============================] - 2s 5ms/step - loss: 0.2060 - accuracy: 0.9256 - val_loss: 0.2521 - val_accuracy: 0.9069\n",
      "Epoch 5/6\n",
      "480/480 [==============================] - 2s 5ms/step - loss: 0.1690 - accuracy: 0.9371 - val_loss: 0.2646 - val_accuracy: 0.9118\n",
      "Epoch 6/6\n",
      "480/480 [==============================] - 3s 6ms/step - loss: 0.1354 - accuracy: 0.9502 - val_loss: 0.2819 - val_accuracy: 0.9055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "second_fashion_model = Sequential()\n",
    "second_fashion_model.add(Conv2D(12,\n",
    "                         activation='relu',\n",
    "                         kernel_size=3,\n",
    "                         input_shape = (img_rows, img_cols, 1)))\n",
    "# Changed kernel sizes to be 2\n",
    "second_fashion_model.add(Conv2D(20, activation='relu', kernel_size=2))\n",
    "second_fashion_model.add(Conv2D(20, activation='relu', kernel_size=2))\n",
    "# added an addition Conv2D layer\n",
    "second_fashion_model.add(Conv2D(20, activation='relu', kernel_size=2))\n",
    "second_fashion_model.add(Flatten())\n",
    "second_fashion_model.add(Dense(100, activation='relu'))\n",
    "# It is important not to change the last layer. First argument matches number of classes. Softmax guarantees we get reasonable probabilities\n",
    "second_fashion_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "second_fashion_model.compile(loss='categorical_crossentropy',\n",
    "                             optimizer='adam',\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "second_fashion_model.fit(x, y, batch_size=100, epochs=6, validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "second_fashion_model.history.history['val_acc'] = second_fashion_model.history.history['val_accuracy']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
